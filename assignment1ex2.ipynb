{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If your first name starts with A-N use the COVID19_mini.csv attached to this assignment, else\n",
    "## use the Artificial_inteligence_mini.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"Artificial_Intelligence_mini.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing: Apply the following:\n",
    "##   a. Load the data into a dataframe. Name it “firstname_df”, and examine the data. You will\n",
    "##   notice that the file has a header and four tweets with their sentiments.\n",
    "##   b. Drop the user column.\n",
    "##   c. Use regular expressions or python string methods to get rid of the additional data at the\n",
    "##   begging and end of each tweet.\n",
    "##   d. Check the tweet data and identify, if you need to carry out any further pre-processing\n",
    "##   steps, you should at least do one or two more steps.\n",
    "##   e. List them in your analysis report under exercise #2.\n",
    "##   f. Cary out those steps in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  \\\n",
      "0  positive   \n",
      "1  negative   \n",
      "2  positive   \n",
      "3  negative   \n",
      "\n",
      "                                                                                                                                           text  \\\n",
      "0  RT @cutedejun: sm not letting xiaojun go for his graduation nor his brother's wedding last year is my villain origin story, he's too nice t…   \n",
      "1   @Marcheline3Di For me, it was spent recovering from an all-nighter I spent on music projects related to a VTuber's… https://t.co/HQ3LEAH1aF   \n",
      "2  RT @OntarioHealthC: 954 ongoing #COVID19 outbreaks in Ontario's hospitals, #LTC and retirement homes. Every day now is a new pandemic high.…   \n",
      "3  RT @MicahPollak: Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga…   \n",
      "\n",
      "              user  \n",
      "0       89SHAWDUHJ  \n",
      "1      Cantomancer  \n",
      "2  RoseAnneReilly5  \n",
      "3  KristinaTraxler  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# a. Load the data into a dataframe. \n",
    "wing_df = pd.read_csv(file, header=0)\n",
    "print(wing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  \\\n",
      "0  positive   \n",
      "1  negative   \n",
      "2  positive   \n",
      "3  negative   \n",
      "\n",
      "                                                                                                                                           text  \n",
      "0  RT @cutedejun: sm not letting xiaojun go for his graduation nor his brother's wedding last year is my villain origin story, he's too nice t…  \n",
      "1   @Marcheline3Di For me, it was spent recovering from an all-nighter I spent on music projects related to a VTuber's… https://t.co/HQ3LEAH1aF  \n",
      "2  RT @OntarioHealthC: 954 ongoing #COVID19 outbreaks in Ontario's hospitals, #LTC and retirement homes. Every day now is a new pandemic high.…  \n",
      "3  RT @MicahPollak: Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga…  \n"
     ]
    }
   ],
   "source": [
    "# b. Drop the user column.\n",
    "wing_df.drop('user', axis=1, inplace=True)\n",
    "print(wing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    RT @cutedejun: sm not letting xiaojun go for his graduation nor his brother's wedding last year is my villain origin story, he's too nice t…\n",
      "1     @Marcheline3Di For me, it was spent recovering from an all-nighter I spent on music projects related to a VTuber's… https://t.co/HQ3LEAH1aF\n",
      "2    RT @OntarioHealthC: 954 ongoing #COVID19 outbreaks in Ontario's hospitals, #LTC and retirement homes. Every day now is a new pandemic high.…\n",
      "3    RT @MicahPollak: Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga…\n",
      "Name: text, dtype: object\n",
      "~~~~~~~~\n",
      "0    sm not letting xiaojun go for his graduation nor his brother's wedding last year is my villain origin story, he's too nice t\n",
      "1                             For me, it was spent recovering from an all-nighter I spent on music projects related to a VTuber's\n",
      "2         954 ongoing #COVID19 outbreaks in Ontario's hospitals, #LTC and retirement homes. Every day now is a new pandemic high.\n",
      "3      Well, #COVID19 is once again the leading cause of death in #Indiana (based on average daily deaths) and closing in on (aga\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# c. get rid of the additional data at the begining and end of each tweet.\n",
    "print(wing_df['text'])\n",
    "string_to_remove_after = \"…\"\n",
    "pattern = f\"{re.escape(string_to_remove_after)}.*\"\n",
    "wing_df['text'] = wing_df['text'].str.replace(pattern, \"\", regex=True)\n",
    "\n",
    "pattern_begin = r'^(?:RT\\s)?@\\w+(?::)?\\s+'\n",
    "wing_df['text'] = wing_df['text'].str.replace(pattern_begin, \"\", regex=True)\n",
    "print('~~~~~~~~')\n",
    "print(wing_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  \\\n",
      "0  positive   \n",
      "1  negative   \n",
      "2  positive   \n",
      "3  negative   \n",
      "\n",
      "                                                                                                                    text  \n",
      "0                              sm letting xiaojun go graduation brothers wedding last year villain origin story hes nice  \n",
      "1                                                       spent recovering allnighter spent music projects related vtubers  \n",
      "2  nine hundred fifty-four ongoing covid19 outbreaks ontarios hospitals ltc retirement homes every day new pandemic high  \n",
      "3                                        well covid19 leading cause death indiana based average daily deaths closing aga  \n"
     ]
    }
   ],
   "source": [
    "# d. Check the tweet data and identify, if you need to carry out any further pre-processing\n",
    "# steps, you should at least do one or two more steps.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import inflect\n",
    "\n",
    "def replace_numbers_with_text(text):\n",
    "    p = inflect.engine()\n",
    "    words = text.split()\n",
    "    replaced_words = [p.number_to_words(word) if word.isdigit() else word for word in words]\n",
    "    return ' '.join(replaced_words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Replace numbers with their textual representations\n",
    "    text = replace_numbers_with_text(text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join tokens back into a sentence\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "wing_df['text'] = wing_df['text'].apply(preprocess_text)\n",
    "print(wing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply the following data augmentation technique to expand the original dataset:\n",
    "\n",
    "Word embedding augmenter. Use “word2vec” as a model for the augmenter. After applying this augmenter, you should have a dataset (call it “firstname_df_after_word_augmenter”). The size of this dataset should be double the size of the original dataset. That is, the size of firstname_df_after_word_augmenter will be 2 X size of the original dataset (firstname_df).\n",
    "\n",
    "a. Augment using random insertion. Write a script to apply the following steps for each cleaned tweet in the dataset:\n",
    "\n",
    "i. Tokenize the cleaned tweet, if you haven’t done so earlier.\n",
    "\n",
    "ii. Remove stop words if you haven’t done so earlier\n",
    "\n",
    "iii. Per tweet choose two words randomly.\n",
    "\n",
    "iv. Get synonyms of each of the words selected in step i\n",
    "\n",
    "v. Select the most similar synonym, and replace the original word with the synonym to create a new tweet (You should not replace the original tweet, you need to add a new copy of each tweet using the selected synonyms to your dataframe as a row and maintain the original sentiment\n",
    "\n",
    "vi. Export the new dataset (after applying random insertion) into a text file. Name it (firstname_df_after_random_insertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [wedding, nice]\n",
      "1          [spent, music]\n",
      "2    [outbreaks, ongoing]\n",
      "3        [cause, leading]\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# i. Tokenize the cleaned tweet, if you haven’t done so earlier. \n",
    "tokens = wing_df['text'].apply(lambda w: word_tokenize(w))\n",
    "\n",
    "# ii. Remove stop words if you haven’t done so earlier\n",
    "# Applied above\n",
    "\n",
    "# iii. Per tweet choose two words randomly.\n",
    "import random\n",
    "random.seed(99)\n",
    "\n",
    "def choose_random_words(words):\n",
    "    if len(words) < 2:\n",
    "        return None\n",
    "    random_words = random.sample(words, 2)\n",
    "    return random_words\n",
    "\n",
    "random_token = tokens.apply(choose_random_words)\n",
    "print(random_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "gn_vec_path = \"/Users/jk/Documents/Documents/sem4/comp262_004_nlp/GoogleNews-vectors-negative300.bin\"\n",
    "# model = Word2Vec(sentences=wing_df['random_token'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "model = KeyedVectors.load_word2vec_format(gn_vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "wedding\n",
      "[('nuptials', 0.7825038433074951), ('weddings', 0.7155418395996094), ('bride', 0.7104349136352539), ('Wedding', 0.6956347227096558), ('bridal_shower', 0.6609436869621277)]\n",
      "----------\n",
      "nice\n",
      "[('good', 0.6836091876029968), ('lovely', 0.6676310896873474), ('neat', 0.6616737246513367), ('fantastic', 0.6569241881370544), ('wonderful', 0.6561347246170044)]\n",
      "----------\n",
      "spent\n",
      "[('spend', 0.7509484887123108), ('spends', 0.7474757432937622), ('Spent', 0.6004622578620911), ('Spend', 0.581665575504303), ('devotes', 0.5402958989143372)]\n",
      "----------\n",
      "music\n",
      "[('classical_music', 0.7197794318199158), ('jazz', 0.683463990688324), ('Music', 0.6595721244812012), ('Without_Donny_Kirshner', 0.6416222453117371), ('songs', 0.6396344900131226)]\n",
      "----------\n",
      "outbreaks\n",
      "[('outbreak', 0.8043086528778076), ('avian_influenza_outbreaks', 0.7017189860343933), ('epidemics', 0.6981160044670105), ('Outbreaks', 0.6815019845962524), ('bird_flu_outbreaks', 0.6654841303825378)]\n",
      "----------\n",
      "ongoing\n",
      "[('continuing', 0.6776866316795349), ('Ongoing', 0.6041178107261658), ('intensifying', 0.5605769157409668), ('continual', 0.5575658679008484), ('onging', 0.5273488759994507)]\n",
      "----------\n",
      "cause\n",
      "[('causes', 0.7839967608451843), ('caused', 0.6618048548698425), ('causing', 0.6376924514770508), ('contributing_factor', 0.5516594648361206), ('Cause', 0.528270959854126)]\n",
      "----------\n",
      "leading\n",
      "[('Leading', 0.6308879852294922), ('preeminent', 0.570842981338501), ('premier', 0.5572081208229065), ('pre_eminent', 0.5500032901763916), ('aleading', 0.522291362285614)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                            [[nuptials, weddings, bride, Wedding, bridal_shower], [good, lovely, neat, fantastic, wonderful]]\n",
       "1                                      [[spend, spends, Spent, Spend, devotes], [classical_music, jazz, Music, Without_Donny_Kirshner, songs]]\n",
       "2    [[outbreak, avian_influenza_outbreaks, epidemics, Outbreaks, bird_flu_outbreaks], [continuing, Ongoing, intensifying, continual, onging]]\n",
       "3                               [[causes, caused, causing, contributing_factor, Cause], [Leading, preeminent, premier, pre_eminent, aleading]]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iv. Get synonyms of each of the words selected in step i\n",
    "\n",
    "def get_synonyms(word, model, topn=1):\n",
    "    try:\n",
    "        print('-'*10)\n",
    "        print(word)\n",
    "        print(model.most_similar(positive=word, topn=topn))\n",
    "        synonyms = [synonym for synonym, coef in model.most_similar(positive=word, topn=topn)]\n",
    "        return synonyms\n",
    "    except KeyError:\n",
    "        return []\n",
    "\n",
    "random_token.apply(lambda words: [get_synonyms(word, model, 5) for word in words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  \\\n",
      "0  positive   \n",
      "1  negative   \n",
      "2  positive   \n",
      "3  negative   \n",
      "4  positive   \n",
      "5  negative   \n",
      "6  positive   \n",
      "7  negative   \n",
      "\n",
      "                                                                                                                      text  \n",
      "0                                sm letting xiaojun go graduation brothers wedding last year villain origin story hes nice  \n",
      "1                                                         spent recovering allnighter spent music projects related vtubers  \n",
      "2    nine hundred fifty-four ongoing covid19 outbreaks ontarios hospitals ltc retirement homes every day new pandemic high  \n",
      "3                                          well covid19 leading cause death indiana based average daily deaths closing aga  \n",
      "4                               sm letting xiaojun go graduation brothers nuptials last year villain origin story hes good  \n",
      "5                                               spend recovering allnighter spend classical_music projects related vtubers  \n",
      "6  nine hundred fifty-four continuing covid19 outbreak ontarios hospitals ltc retirement homes every day new pandemic high  \n",
      "7                                         well covid19 Leading causes death indiana based average daily deaths closing aga  \n"
     ]
    }
   ],
   "source": [
    "# v. Select the most similar synonym, and replace the original word with the synonym to create a new tweet \n",
    "wing_df_after_word_augmenter = wing_df.copy()\n",
    "temp = []\n",
    "for idx, row in wing_df.iterrows():\n",
    "    sentiment = row['sentiment']\n",
    "    text = row['text']\n",
    "    token1 = random_token[idx][0]\n",
    "    syn1 = model.most_similar(positive=token1, topn=1)[0][0]\n",
    "    token2 = random_token[idx][1]\n",
    "    syn2 = model.most_similar(positive=token2, topn=1)[0][0]\n",
    "    text = text.replace(token1, syn1).replace(token2, syn2)\n",
    "    temp.append([sentiment, text])\n",
    "\n",
    "df2 = pd.DataFrame(temp, columns=wing_df.columns)\n",
    "wing_df_after_word_augmenter = pd.concat([wing_df_after_word_augmenter,df2], ignore_index=True)\n",
    "print(wing_df_after_word_augmenter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# vi. Export the new dataset (after applying random insertion) into a text file. Name it (firstname_df_after_random_insertion)\n",
    "import sys\n",
    "with open('wing_df_after_random_insertion.txt', 'w') as file:\n",
    "    file.write(wing_df_after_word_augmenter.to_string())\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
